\chapter{Apprentissage séquence à séquence}

Les modèles ``séquence à séquence'' sont une famille d'algorithmes de \acrfull{ml}
dont l'entrée et la sortie sont des séquences.
% Formellement, ils prennent en entrée une séquence \(x = (x_1, x_2,\cdots, x_n)\) 
% et produisent une séquence \(y = (y_1, y_2,\cdots, y_m)\).
% Dans le cas général, \(n\neq m\) et aucune hypothèse d'alignement temporel n'est supposée.
Plusieurs tâches de \acrlong{ml}, notamment en \acrfull{nlp}, 
peuvent être formulées comme tâches d'apprentissage séquence à séquence.
Parmi ces tâches, nous citons : la création de chatbots, la réponse aux questions, 
la reconnaissance automatique de la parole et la \acrlong{mt}.


\section{Réseaux de neurones récurrents (\acrshort{rnn})}

Le \acrshort{rnn} (\Acrlong{rnn}) est une architecture de réseau de neurone 
conçue pour la modélisation des séquences.
Elle se base sur l'idée de boucle de rétroaction pour capturer les dépendances temporelles.

