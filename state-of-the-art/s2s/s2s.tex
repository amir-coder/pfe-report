\chapter{Apprentissage séquence à séquence}

Les modèles ``séquence à séquence'' sont une famille d'algorithmes de \acrfull{ml}
dont l'entrée et la sortie sont des séquences.
% Formellement, ils prennent en entrée une séquence \(x = (x_1, x_2,\cdots, x_n)\) 
% et produisent une séquence \(y = (y_1, y_2,\cdots, y_m)\).
% Dans le cas général, \(n\neq m\) et aucune hypothèse d'alignement temporel n'est supposée.
Plusieurs tâches de \acrlong{ml}, notamment en \acrfull{nlp}, 
peuvent être formulées comme tâches d'apprentissage séquence à séquence.
Parmi ces tâches, nous citons : la création de chatbots, la réponse aux questions, 
la reconnaissance automatique de la parole et la \acrlong{mt}.

Dans ce chapitre, nous commençons par formuler le problème de modélisation de séquences.
En suite, nous présentons les architectures neuronales les plus utilisées pour cette tâche.
En fin, nous terminons avec une étude comparative de celles-ci.

\subimport{}{problem}
\subimport{}{rnn}

