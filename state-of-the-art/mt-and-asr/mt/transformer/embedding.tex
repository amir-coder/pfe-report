\subsubsection{Plongement lexical}

Le transformeur --- comme tout réseau de neurones --- n'opère que sur des vecteurs.
Pour lui faire passer des phrases, il faut donc les transformer en vecteurs.
La première étape consiste à transformer les mots en vecteurs.
C'est ce qu'on appelle un \emph{plongement lexical}~\cite{Almeida_Xexéo_2019}.

La première étape dans le calcul des plongements lexicaux est de diviser le texte en unités lexicales (tokens).
Il peut s'agir de mots, d'ensembles de mots ou d'unités plus petites qu'un mot.
La sortie de cette étape est appelée un \emph{vocabulaire}.
Plusieurs techniques existent pour effectuer ce découpage. 
La plus simple et d'assimiler chaque mot à un token,
mais d'autres techniques comme le \gls{bpe} proposent la segmentation des mots en sous-éléments.
Le choix des sous-mots à garder dans le vocabulaire est le plus souvent une fonction de la fréquence dans le corpus%
~\cite{Rai_Borah_2021}.

Une fois le vocabulaire construit, il lui faut associer un plongement.
L'application qui mappe les tokens sur les vecteurs peut avoir une implémentation arbitraire.
Il peut s'agir d'un simple tableau de vecteurs~\cite{Paszke_et_al_2019},
comme il peut s'agir d'un réseau de neurones~\cite{Church_2017}.
Une fois qu'on a mis la phrase sous forme vectorielle,
le transformeur peut la traiter (voir Section~\ref{sec.transformer}).