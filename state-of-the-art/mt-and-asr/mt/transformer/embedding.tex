\subsubsection{Plongement lexical}

Le transformeur --- comme tout réseau de neurones --- n'opère que sur des vecteurs.
Pour lui faire passer des phrases, il faut donc les transformer en vecteurs.
La première étape consiste à transformer les mots en vecteurs.
C'est ce qu'on appelle un \emph{plongement lexical}~\cite{Almeida_Xexéo_2019}.

La première étape dans le calcul des plongements lexicaux est de diviser le texte en unités lexicales (tokens).
Il peut s'agir de mots, d'ensembles de mots ou d'unités plus petites qu'un mot.
La sortie de cette étape est appelée un \emph{vocabulaire}.
Plusieurs techniques existent pour effectuer ce découpage. 
La plus simple et d'assimiler chaque mot à un token,
mais d'autres techniques comme le \gls{bpe} proposent la segmentation des mots en sous-éléments.
Le choix des sous-mots à garder dans le vocabulaire est le plus souvent une fonction de la fréquence dans le corpus%
~\cite{Rai_Borah_2021}.


Après l'avoir construit, un plongement doit être associé au vocabulaire.
L'application qui mappe les tokens sur les vecteurs peut avoir une implémentation arbitraire.
Il peut s'agir d'un simple tableau de vecteurs~\cite{Paszke_et_al_2019},
comme il peut s'agir d'un réseau de neurones~\cite{Church_2017}.
L'objectif est d'associer aux tokens similaires des vecteurs similaires%
\footnote{Qui ont des grands produits scalaires.}, %
on ramène ainsi, la relation mal définie de similarité entre les mots à une relation bien définie sur les vecteurs.
Une fois qu'on a mis la phrase sous forme vectorielle,
le transformeur peut la traiter (voir Section~\ref{sec.transformers}).