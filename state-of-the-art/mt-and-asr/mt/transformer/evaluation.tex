\subsubsection{Évaluation et métriques}

Plusieurs métriques peuvent être utilisées pour évaluer la qualité d'une traduction automatique.
Les plus simples sont les métriques fondées sur la matrice de confusion (notamment la précision et le rappel).
Ces métriques sont conçues pour quantifier la qualité des classifications simples.
Cela les rend inadaptées à la \gls{mt}.
Par exemple, la précision de la traduction candidate ``Le le le le'' 
par rapport à la référence ``Le chat est sur le tapis'' est égale à \(1\).

Des métriques plus adaptées à la \gls{mt} ont été proposées.
La plus connue parmi elles est \gls{bleu}~\cite{Papineni_Roukos_Ward_Zhu_2002}.
\gls{bleu} prend la précision comme point de départ.
Il l'améliore en apportant deux modifications :
\begin{enumerate*}
    \item il considère la précision sur les \(n-\)grammes%
    \footnote{séquences de \(n\) mots consécutifs.} %
    plutôt que sur les mots individuels et
    \item il prend en compte le nombre de fois où un \(n-\)gramme est présent dans la référence.
\end{enumerate*}
Le résultat est appelé \emph{la précision \(n-\)gramme modifiée}.
Il est donné par la formule suivante%
\footnote{%
    \(G_n\left(s\right)\) est l'ensemble des \(n-\)grammes de \(s\) et \(\#(s, s^\prime)\) est le nombre d'occurrences de \(s^\prime\) dans \(s\).} :%
\begin{equation}
    \label{eq:mod-ng-prec}
    p_n\left(\hat{y}, y\right) = 
    \frac{\sum\limits_{g \in G_n\left(\hat{y}\right)} \min{\left(\#(\hat{y}, g), \#(y, g)\right)}}
    {\sum\limits_{g \in G_n\left(\hat{y}\right)} \#(\hat{y}, g)}
\end{equation}
\gls{bleu} est obtenu en prenant la moyenne géométrique des \(p_n\) 
multipliée par une pénalité exponentielle pour les phrases trop courtes (voir Algorithme~\ref{algo.bleu}).

\lstinputlisting[
    float={hbt},
    language=Python,
    firstnumber=1,
    firstline=4,
    label={algo.bleu},
    caption={Score \glsfmtshort{bleu}.},
]{assets/code/bleu.py}