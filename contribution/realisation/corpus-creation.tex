\section{Création des corpus}%
\label{sec.corpus-creation}

Dans cette section, nous donnons les détails de l'implémentation de la première étape de notre solution, 
à savoir la création des corpus.
Nous commençons par le corpus de la partie \gls{asr}.
Ensuite, nous passons à celui de la partie \gls{mt}.

\subsection{\Glsfmtlong{asr}}%
\label{subsec.asr-corpus-creation}

Pour la création du corpus de la partie \gls{asr}, 
nous avons commencé par repérer les vidéos pertinentes sur YouTube et Vimeo.
Le résultat de cette opération est une liste de 3 vidéos qui n'ont pas déjà été transcrites.

Après cela, nous avons divisé chaque vidéo en plusieurs segments en fonction de la personne qui parle.
Nous avons organisé ces segments en un fichier \verb|url.yaml| dont un extrait est donné ci-dessous.
\begin{verbatim}
allo-docteurs:
    url: https://www.youtube.com/watch?v=rqoSKafN3aw
    is_broca: yes
    segments:
        jean-dominique:
            - [2:00, 5:40]
            - [8:20, 10:02]
            - [16:36, 18:00]
            - [20:36, 22:48]
            - [23:17, 23:39]
      raquel:
            - [10:20, 11:20]
            - [11:37, 11:52]
            - [12:24, 13:50]
hug:
    url: https://www.youtube.com/watch?v=d4Cybwx3sHk
    is_broca: yes
    segments:
        marie:
            - [34, 1:07]
            - [6:42, 6:47]
        jean-pierre:
            - [7:57, 8:57]
\end{verbatim}

Une fois préparé, ce fichier est passé à un script Python 
qui se charge de télécharger les vidéos (en utilisant \verb|youtube-dl|) et de les découper en segments.
Ces segments sont ensuite transcrits en utilisant l'interface ligne de commande de Whisper.

\subsection{\Glsfmtlong{mt}}%
\label{subsec.mt-corpus-creation}

Pour la création du corpus de la partie \gls{mt},
nous avons interrogé l'\gls{abb.api} d'\foreignlanguage{english}{Open AI}, 
plus précisément, le modèle \verb|gpt-3.5-turbo| qui est celui derrière chatGPT
(voir Extrait de code~\ref{code.generate-errors}).
Nous avons passé 3 paramètres à chatGPT :
\begin{itemize}
    \item \verb|messages| : une liste d'interactions décrivant l'histoire de la conversation.
    Chaque interaction est un dictionnaire contenant les clés \verb|role| et \verb|content|.
    La clé \verb|role| représente l'identité de l'interlocuteur.
    Elle admet 3 valeurs : \verb|user|, \verb|bot| et \verb|system|.
    Le message \verb|system| est utilisé pour configurer le comportement de chatGPT pour le reste de la conversation.
    \item \verb|timeout| : le temps maximal que chatGPT peut prendre pour générer une réponse (en secondes).
    \item \verb|n| : le nombre de réponses à générer.
\end{itemize}
Cette opération peut échouer à cause des limitations de l'\gls{abb.api} d'\foreignlanguage{english}{Open AI}.
Si le nombre de requêtes dépasse le quota autorisé, l'\gls{abb.api} renvoie une erreur.
Dans ce cas, nous attrapons l'erreur et nous réessayons après une seconde.
Dans le cas contraire, nous passons au mot suivant.
À la fin de la boucle, les erreurs sont enregistrées dans un fichier \verb|errors.yaml|.

\lstinputlisting[
    language=Python,
    caption={Génération des erreurs avec chatGPT.},
    label={code.generate-errors},
    style=mystyle,
]{assets/scripts/generate.py}

Le fichier \verb|errors.yaml| est combiné avec le corpus pour produire un corpus parallèle synthétique.
Cette procédure est illustrée par l'extrait de code~\ref{code.corrupt}.
Pour chaque phrase, les mots modifiables sont mis dans une liste \verb|corruptable_words|.
À partir de cette liste, une deuxième liste \verb|corruptions| est générée qui contient 
toutes les combinaisons de modifications possibles.
Ces modifications sont donc appliquées à la phrase d'origine pour produire des phrases corrompues.
Les phrases corrompues (différentes de la phrase d'origine) sont ajoutées au corpus parallèle qui est retourné.

\lstinputlisting[
    language=Python,
    caption={Création du corpus parallèle synthétique.},
    label={code.corrupt},
]{assets/scripts/corrupt.py}

Il est important de souligner que les erreurs sont  divisées en 3 parties 
(entraînement, validation et test) avant d'être combinées avec le corpus.
Cela a pour but d'éviter le sur-apprentissage à cause de fuites de données,
c'est-à-dire que les règles de modification soient les mêmes dans les 3 parties
même si les phrases sont différentes.