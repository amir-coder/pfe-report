\subsection{Analyse comparative de la performance}
\label{subsec.performance}

Dans le début de cette section, nous affirmons la supériorité du transformeur aux autres architectures~\gls{s2s}.
Ce constat repose sur la comparaison du Tableau~\ref{tab.performance}~\cite{attention}.
On y voit que le transformeur est le seul à avoir une longueur de chemin constante,
c'est-à-dire que cette longueur possède un majorant indépendant de la taille de l'entrée.

On note également que le transformeur exige un nombre d'opérations séquentielles constant,
ce qui est le cas pour les autres architectures à l'exception des \glspl{rnn}.
En effet, tous les réseaux \foreignlanguage{english}{feed-forward} ont cette propriété.

Pour la complexité par couches, l'auto-attention est la plus efficace pour les courtes séquences
(pour lesquelles \(n \ll d\)).
Ces séquences sont les plus fréquentes en pratique pour les dimensions de représentation usuelles.
Dans le cas de très longues séquences, elle est moins efficace que les autres architectures.
Cependant, le transformeur donne toujours de meilleurs résultats pour ces séquences~\cite{Shim_Sung_2022}.


\begin{table}[htb]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Type du module  
        & Complexité       
        & \stackanchor{Nombre d'Opérations}{Séquentielles} 
        & \stackanchor{Longueur du Chemin}{Emprunté par le Gradient}  \\
        \hline
        Auto-Attention & \(\bigO{n^2 \cdot d}\)         & \(\bigO{1}\) & \(\bigO{1}\)        \\
        Recurrent      & \(\bigO{n \cdot d^2}\)         & \(\bigO{n}\) & \(\bigO{n}\)        \\
        Convolutif     & \(\bigO{k \cdot n \cdot d^2}\) & \(\bigO{1}\) & \(\bigO{\log_k n}\) \\
        \gls{mlp}      & \(\bigO{n \cdot d^2}\)         & \(\bigO{1}\) & \(+\infty\)     \\
        \bottomrule
    \end{tabular}
    \caption{%
      Analyse comparative de la complexité des différents types d'architectures \glsfmtshort{s2s}.
      \(n\) est la longueur de la séquence, 
      \(d\) est la dimension de la représentation vectorielle des éléments 
      et \(k\) est la taille du noyau des convolutions~\cite{attention}.
    }
  \label{tab.performance}
\end{table}
