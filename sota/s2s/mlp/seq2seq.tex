% todo: add citation
\subsection{Application à la modélisation de séquence}

Les réseaux de neurones opèrent sur des vecteurs.
Afin de les utiliser dans le contexte de la modélisation \gls{s2s}, 
il faut utiliser une représentation vectorielle des entrées.
Une telle représentation s'appelle un \emph{plongement} (\foreignlanguage{english}{embedding} en anglais).
Le plongement peut-être appris ou prédéfini~\cite{Raschka_Mirjalili_2017}.

Dans le cas des \glspl{mlp}, la séquence d'entrée est d'abord décomposée en sous-séquences.
Ensuite, les plongements de ces sous-séquences sont traités un par un par le réseau de neurones,
ce qui produit une séquence de vecteurs en sortie.
(voir Algorithme~\ref{algo.mlp-pass}).

\lstinputlisting[
    language=python, 
    caption={Passe d'un \glsfmtshort{mlp}}, 
    label={algo.mlp-pass}, 
    firstline=9,
    firstnumber=1
]{assets/code/mlp.py}

% \begin{algorithm}[htb]
%     \caption{Passe d'un \glsfmtshort{mlp}}
%     \label{algo.mlp-pass}
%     \SetKwFunction{embedding}{embedding}
%     \SetKwFunction{sub}{sub\_sequences}
%     \SetKwFunction{mlp}{FFN}
%     % \DontPrintSemicolon
%     \KwIn{input\_sequence \comment{Séquence d'entrée}}
%     \Begin{
%         \(y \gets \emptyset\)\;
%         \ForEach{\(x\in\sub{input\_sequence}\)}{
%             \(\tilde{x} \gets \embedding{x}\)\;
%             \(y \gets y \cup\)\mlp{\(\tilde{x}\)}\;
%         }
%     }
%     \KwOut{\(y\) \comment{Séquence de sortie}}
% \end{algorithm}
