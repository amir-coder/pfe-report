\subsection{Généralités}

Dans cette section, nous introduisons les \glspl{mlp},
l'architecture neuronale la plus simple et la plus utilisée.
Il s'agit d'une simple composition de couches affines avec des activations non affines 
(voir Définition~\ref{def.mlp}).


\begin{definition}[\gls{mlp}]\ \\
    \label{def.mlp}
    Soient \(k, w_0, w_1, \cdots, w_{k+1}\in\naturals\), 
    un réseau de neurones feed-forward de profondeur \(k+1\), à \(w_0\) entrées et \(w_{k+1}\) sorties, 
    est défini par une fonction :
    \begin{equation}
        \label{eq.mlp}
        \left\{
        \begin{array}{l}
            \reals^{w_0} \to \reals^{w_{k+1}}\\
            x\mapsto
            \phi_{k+1} \circ A_{k+1} \circ \phi_k \circ A_k \circ \cdots \circ \phi_1 \circ A_1(x)
        \end{array}
        \right.
    \end{equation}
    où les \(A_i\) sont des fonctions affines \(\reals^{w_{i-1}}\to\reals^{w_i}\) 
    et les \(\phi_i\) sont des fonctions quelconques, typiquement non affines
    \(\reals^{w_i}\to\reals^{w_i}\), dites \emph{d'activations}.
    La fonction \(\phi_i\circ A_i\) est appelée la \(i^{\, \mathrm{eme}}\) couche du réseau%
    ~\cite{Mukherjee_2021}.
\end{definition}

À un tel réseau de neurones, on peut associer un graphe orienté acyclique 
qu'on appelle son ``architecture sous-jacente''~\cite{Kearns_Vazirani_1994}. 
La Figure~\ref{fig.mlp} illustre l'architecture d'un \gls{mlp} de profondeur 4
avec \({(w_0, w_1, w_2, w_3, w_4) = (4, 5, 7, 5, 4)}\).

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=10cm]{assets/images/mlp.png}
    \end{center}
    \caption{Architecture sous-jacente d'un \glsfmtshort{mlp} de profondeur 4.}
    \label{fig.mlp}
\end{figure}

Deux \glspl{mlp} peuvent avoir la même architecture sous-jacente,
en effet, cette dernière ne dépend que des dimensions de leurs couches respectives.
De ce fait, une méthode de trouver, pour une architecture et une fonction cible données,
le meilleur \gls{mlp} est nécessaire~\cite{Mukherjee_2021}.

Pour ce faire, nous exploitons le fait que 
les \(A_i\) sont des applications affines sur des espaces de dimensions finies.
Nous pouvons donc les écrire comme combinaisons de produits matriciels et de translations.
Le problème se réduit donc à régler \footnote{En \gls{ml}, le terme ``entraîner'' est plutôt utilisé.}
les paramètres des matrices en question. 
Cela nécessite une façon de quantifier la qualité d'approximation d'une fonction \(f\) par une autre \(\hat{f}\).
L'analyse fonctionnelle nous en donne plusieurs. 
Les~\cref{eq.l1-norm,eq.l2-norm} sont deux exemples récurrents de fonctions dites \emph{de perte}.

\begin{equation}
    \label{eq.l1-norm}
    L_1(f, \hat{f}) = \left\|f - \hat{f}\right\|_1 = \int \left|f - \hat{f}\right|
\end{equation}

\begin{equation}
    \label{eq.l2-norm}
    L_2(f, \hat{f}) = \left\|f - \hat{f}\right\|_2^2 = \int \left|f - \hat{f}\right|^2
\end{equation}

Ayant fixé une fonction de perte \(L\), 
l'entraînement revient à un problème d'optimisation comme représenté par l'équation~\ref{eq.learn-opt}.
\begin{equation}
    \label{eq.learn-opt}
    f^\ast = \argmin_{\hat{f}}{L(f, \hat{f})}
\end{equation}
Dans le cas particulier où \(L\) est différentiable, 
l'algorithme du gradient peut être utilisé pour trouver un minimum local.
Les gradients sont calculés en utilisant une méthode de dérivation automatique comme la rétro-propagation.
